{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "char_rnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "h4T2cBVI0s1g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# A Char-RNN Implementation in Tensorflow\n",
        "---\n",
        "CharRNN was a well known generative text model (character level LSTM) created by Andrej Karpathy. It allowed easy training and generation of arbitrary text with many hilarious results:\n",
        "\n",
        "  * Music: abc notation\n",
        "<https://highnoongmt.wordpress.com/2015/05/22/lisls-stis-recurrent-neural-networks-for-folk-music-generation/>,\n",
        "  * Irish folk music\n",
        "<https://soundcloud.com/seaandsailor/sets/char-rnn-composes-irish-folk-music>-\n",
        "  * Obama speeches\n",
        "<https://medium.com/@samim/obama-rnn-machine-generated-political-speeches-c8abd18a2ea0>-\n",
        "  * Eminem lyrics\n",
        "<https://soundcloud.com/mrchrisjohnson/recurrent-neural-shady>- (NSFW ;-))\n",
        "  * Research awards\n",
        "<http://karpathy.github.io/2015/05/21/rnn-effectiveness/#comment-2073825449>-\n",
        "  * TED Talks\n",
        "<https://medium.com/@samim/ted-rnn-machine-generated-ted-talks-3dd682b894c0>-\n",
        "  * Movie Titles <http://www.cs.toronto.edu/~graves/handwriting.html>\n",
        "  \n",
        "This notebook contains a reimplementation in Tensorflow. It will let you input a file containing the text you want your generator to mimic, train your model, see the results, and save it for future use.\n",
        "\n",
        "To get started, start running the cells in order, following the instructions at each step. You will need a sizable text file (try at least 1 MB of text) when prompted to upload one. For exploration you can also use the provided text corpus taken from Shakespeare's works.\n",
        "\n",
        "The training cell saves a checkpoint every 30 seconds, so you can check the output of your network and not lose any progress.\n",
        "\n",
        "## Outline\n",
        "\n",
        "This notebook will guide you through the following steps. Roughly speaking, these will be our steps: \n",
        "  * Upload some data\n",
        "  * Set some training parameters (you can just use the defaults for now)\n",
        "  * Define our Model, training loss function, and data input manager\n",
        "  * Train on a cloud GPU\n",
        "  * Save out model and use it to generate some new text.\n",
        "  \n",
        "Design of the RNN is inspired by [this github project](https://github.com/sherjilozair/char-rnn-tensorflow) which was based on Andrej Karpathy's [char-rnn](https://github.com/karpathy/char-rnn). If you'd like to learn more, Andrej's [blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a great place to start."
      ]
    },
    {
      "metadata": {
        "id": "NhBCvDKnVstE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Imports and Values Needed to Run this Code"
      ]
    },
    {
      "metadata": {
        "id": "67qDGHYCzj6v",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, print_function, division\n",
        "from google.colab import files\n",
        "from collections import Counter, defaultdict\n",
        "from copy import deepcopy\n",
        "from IPython.display import clear_output\n",
        "from random import randint\n",
        "\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "CHECKPOINT_DIR = './checkpoints/'  #Checkpoints are temporarily kept here.\n",
        "TEXT_ENCODING = 'utf-8'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mBtu02V2ocXC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Get the training data.\n",
        "\n",
        "We can either download the works of Shakespeare to train on or upload our own plain text file that we will be training on."
      ]
    },
    {
      "metadata": {
        "id": "FvoepQadkPG6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "1d95d0c6-cf34-4e0e-b683-f922b5178d25"
      },
      "cell_type": "code",
      "source": [
        "shakespeare_url = \"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\"\n",
        "import urllib\n",
        "file_contents = urllib.urlopen(shakespeare_url).read()\n",
        "file_name = \"shakespeare\"\n",
        "file_contents = file_contents[10501:]  # Skip headers and start at content\n",
        "print(\"An excerpt: \\n\", file_contents[:664])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "An excerpt: \n",
            "                      1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But as the riper should by time decease,\n",
            "  His tender heir might bear his memory:\n",
            "  But thou contracted to thine own bright eyes,\n",
            "  Feed'st thy light's flame with self-substantial fuel,\n",
            "  Making a famine where abundance lies,\n",
            "  Thy self thy foe, to thy sweet self too cruel:\n",
            "  Thou that art now the world's fresh ornament,\n",
            "  And only herald to the gaudy spring,\n",
            "  Within thine own bud buriest thy content,\n",
            "  And tender churl mak'st waste in niggarding:\n",
            "    Pity the world, or else this glutton be,\n",
            "    To eat the world's due, by the grave and thee.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nMOW-k6sUgS9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you want to train on your own training data, run the next two cells. Otherwise skip them."
      ]
    },
    {
      "metadata": {
        "id": "B7anKDCqMkrQ",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2c7b9bbf-6d4e-4b6f-ac3b-e75073e43311"
      },
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3a809afd-41fd-4ee6-8505-e4679e54c4b2\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-3a809afd-41fd-4ee6-8505-e4679e54c4b2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"</script> "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "ZtCtMN7FoZYo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "8736e527-63a3-4cc0-ab90-77b1cb710698"
      },
      "cell_type": "code",
      "source": [
        "if uploaded:\n",
        "  if type(uploaded) is not dict: uploaded = uploaded.files  ## Deal with filedit versions\n",
        "  file_bytes = uploaded[uploaded.keys()[0]]\n",
        "  utf8_string = file_bytes.decode(TEXT_ENCODING)\n",
        "  file_contents = utf8_string if files else ''\n",
        "  file_name = uploaded.keys()[0]\n",
        "print(\"An excerpt: \\n\", file_contents[:664])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "An excerpt: \n",
            "                      1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But as the riper should by time decease,\n",
            "  His tender heir might bear his memory:\n",
            "  But thou contracted to thine own bright eyes,\n",
            "  Feed'st thy light's flame with self-substantial fuel,\n",
            "  Making a famine where abundance lies,\n",
            "  Thy self thy foe, to thy sweet self too cruel:\n",
            "  Thou that art now the world's fresh ornament,\n",
            "  And only herald to the gaudy spring,\n",
            "  Within thine own bud buriest thy content,\n",
            "  And tender churl mak'st waste in niggarding:\n",
            "    Pity the world, or else this glutton be,\n",
            "    To eat the world's due, by the grave and thee.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TO0NDmM0VgQU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Set up the recurrent LSTM network \n",
        "\n",
        "Before we can do anything, we have to define what our neural network looks like. This next cell creates a class which will contain the tensorflow graph and training parameters that make up the network."
      ]
    },
    {
      "metadata": {
        "id": "UAK1D26NKGpJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RNN(object):\n",
        "  \"\"\"Represents a Recurrent Neural Network using LSTM cells.\n",
        "\n",
        "  Attributes:\n",
        "    num_layers: The integer number of hidden layers in the RNN.\n",
        "    state_size: The size of the state in each LSTM cell.\n",
        "    num_classes: Number of output classes. (E.g. 256 for Extended ASCII).\n",
        "    batch_size: The number of training sequences to process per step.\n",
        "    sequence_length: The number of chars in a training sequence.\n",
        "    batch_index: Index within the dataset to start the next batch at.\n",
        "    on_gpu_sequences: Generates the training inputs for a single batch.\n",
        "    on_gpu_targets: Generates the training labels for a single batch.\n",
        "    input_symbol: Placeholder for a single label for use during inference.\n",
        "    temperature: Used when sampling outputs. A higher temperature will yield\n",
        "      more variance; a lower one will produce the most likely outputs. Value\n",
        "      should be between 0 and 1.\n",
        "    initial_state: The LSTM State Tuple to initialize the network with. This\n",
        "      will need to be set to the new_state computed by the network each cycle.\n",
        "    logits: Unnormalized probability distribution for the next predicted\n",
        "      label, for each timestep in each sequence.\n",
        "    output_labels: A [batch_size, 1] int32 tensor containing a predicted\n",
        "      label for each sequence in a batch. Only generated in infer mode.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               rnn_num_layers=1,\n",
        "               rnn_state_size=128,\n",
        "               num_classes=256,\n",
        "               rnn_batch_size=1,\n",
        "               rnn_sequence_length=1):\n",
        "    self.num_layers = rnn_num_layers\n",
        "    self.state_size = rnn_state_size\n",
        "    self.num_classes = num_classes\n",
        "    self.batch_size = rnn_batch_size\n",
        "    self.sequence_length = rnn_sequence_length\n",
        "    self.batch_shape = (self.batch_size, self.sequence_length)\n",
        "    print(\"Built LSTM: \",\n",
        "          self.num_layers ,self.state_size ,self.num_classes ,\n",
        "          self.batch_size ,self.sequence_length ,self.batch_shape)\n",
        "\n",
        "\n",
        "  def build_training_model(self, dropout_rate, data_to_load):\n",
        "    \"\"\"Sets up an RNN model for running a training job.\n",
        "\n",
        "    Args:\n",
        "      dropout_rate: The rate at which weights may be forgotten during training.\n",
        "      data_to_load: A numpy array of containing the training data, with each\n",
        "        element in data_to_load being an integer representing a label. For\n",
        "        example, for Extended ASCII, values may be 0 through 255.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: If mode is data_to_load is None.\n",
        "    \"\"\"\n",
        "    if data_to_load is None:\n",
        "      raise ValueError('To continue, you must upload training data.')\n",
        "    inputs = self._set_up_training_inputs(data_to_load)\n",
        "    self._build_rnn(inputs, dropout_rate)\n",
        "\n",
        "  def build_inference_model(self):\n",
        "    \"\"\"Sets up an RNN model for generating a sequence element by element.\n",
        "    \"\"\"\n",
        "    self.input_symbol = tf.placeholder(shape=[1, 1], dtype=tf.int32)\n",
        "    self.temperature = tf.placeholder(shape=(), dtype=tf.float32,\n",
        "                                      name='temperature')\n",
        "    self.num_options = tf.placeholder(shape=(), dtype=tf.int32,\n",
        "                                      name='num_options')\n",
        "    self._build_rnn(self.input_symbol, 0.0)\n",
        "\n",
        "    self.temperature_modified_logits = tf.squeeze(\n",
        "        self.logits, 0) / self.temperature\n",
        "\n",
        "    #for beam search\n",
        "    self.normalized_probs = tf.nn.softmax(self.logits)\n",
        "\n",
        "    self.output_labels = tf.multinomial(self.temperature_modified_logits,\n",
        "                                        self.num_options)\n",
        "\n",
        "  def _set_up_training_inputs(self, data):\n",
        "    self.batch_index = tf.placeholder(shape=(), dtype=tf.int32)\n",
        "    batch_input_length = self.batch_size * self.sequence_length\n",
        "\n",
        "    input_window = tf.slice(tf.constant(data, dtype=tf.int32),\n",
        "                            [self.batch_index],\n",
        "                            [batch_input_length + 1])\n",
        "\n",
        "    self.on_gpu_sequences = tf.reshape(\n",
        "        tf.slice(input_window, [0], [batch_input_length]), self.batch_shape)\n",
        "\n",
        "    self.on_gpu_targets = tf.reshape(\n",
        "        tf.slice(input_window, [1], [batch_input_length]), self.batch_shape)\n",
        "\n",
        "    return self.on_gpu_sequences\n",
        "\n",
        "  def _build_rnn(self, inputs, dropout_rate):\n",
        "    \"\"\"Generates an RNN model using the passed functions.\n",
        "\n",
        "    Args:\n",
        "      inputs: int32 Tensor with shape [batch_size, sequence_length] containing\n",
        "        input labels.\n",
        "      dropout_rate: A floating point value determining the chance that a weight\n",
        "        is forgotten during evaluation.\n",
        "    \"\"\"\n",
        "    # Alias some commonly used functions\n",
        "    dropout_wrapper = tf.contrib.rnn.DropoutWrapper\n",
        "    lstm_cell = tf.contrib.rnn.LSTMCell\n",
        "    multi_rnn_cell = tf.contrib.rnn.MultiRNNCell\n",
        "\n",
        "    self._cell = multi_rnn_cell(\n",
        "        [dropout_wrapper(lstm_cell(self.state_size), 1.0, 1.0 - dropout_rate)\n",
        "         for _ in range(self.num_layers)])\n",
        "\n",
        "    self.initial_state = self._cell.zero_state(self.batch_size, tf.float32)\n",
        "\n",
        "    embedding = tf.get_variable('embedding',\n",
        "                                [self.num_classes, self.state_size])\n",
        "\n",
        "    embedding_input = tf.nn.embedding_lookup(embedding, inputs)\n",
        "    output, self.new_state = tf.nn.dynamic_rnn(self._cell, embedding_input,\n",
        "                                               initial_state=self.initial_state)\n",
        "\n",
        "    self.logits = tf.contrib.layers.fully_connected(output, self.num_classes,\n",
        "                                                    activation_fn=None)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LVEdNYclTSdv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Let's define our training parameters.\n",
        "Feel free to leave these untouched at their default values and just run this cell as is. Later, you can come back here and experiment wth these. \n",
        "These parameters are just for training. Further down at the inference step, we'll define parameters for the text-generation step."
      ]
    },
    {
      "metadata": {
        "id": "qRPTh7_A2u80",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "cell_type": "code",
      "source": [
        "num_layers = 2\n",
        "state_size = 256\n",
        "batch_size = 64\n",
        "sequence_length = 256\n",
        "num_training_steps = 30000 # Takes about 40 minuets \n",
        "steps_per_epoch = 500\n",
        "learning_rate = 0.002\n",
        "learning_rate_decay = 0.95\n",
        "gradient_clipping = 5.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JJ2NdzM5RHyK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Define your loss function\n",
        "Loss is a measure of how well the neural network is modeling the data distribution. \n",
        "\n",
        "Pass in your logits and the targets you're training against. In this case, target_weights is a set of multipliers that will put higher emphasis on certain outputs. In this notebook, we'll give all outputs equal importance."
      ]
    },
    {
      "metadata": {
        "id": "gbBZuBT6NhVG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_loss(logits, targets, target_weights):\n",
        "  with tf.name_scope('loss'):\n",
        "    return tf.contrib.seq2seq.sequence_loss(\n",
        "        logits,\n",
        "        targets,\n",
        "        target_weights,\n",
        "        average_across_timesteps=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Svir_HYvpHQn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define your optimizer\n",
        "This tells Tensorflow how to reduce the loss. We will use the popular [ADAM algorithm](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer)"
      ]
    },
    {
      "metadata": {
        "id": "swLcZqsePGAG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_optimizer(loss, initial_learning_rate, gradient_clipping, global_step,\n",
        "                  decay_steps, decay_rate):\n",
        "\n",
        "  with tf.name_scope('optimizer'):\n",
        "    computed_learning_rate = tf.train.exponential_decay(\n",
        "        initial_learning_rate,\n",
        "        global_step,\n",
        "        decay_steps,\n",
        "        decay_rate,\n",
        "        staircase=True)\n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer(computed_learning_rate)\n",
        "    trained_vars = tf.trainable_variables()\n",
        "    gradients, _ = tf.clip_by_global_norm(\n",
        "        tf.gradients(loss, trained_vars),\n",
        "        gradient_clipping)\n",
        "    training_op = optimizer.apply_gradients(\n",
        "        zip(gradients, trained_vars),\n",
        "        global_step=global_step)\n",
        "\n",
        "    return training_op, computed_learning_rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "60O5Dw_Hr5-s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### This class will let us view the progress of our training as it progresses."
      ]
    },
    {
      "metadata": {
        "id": "lmfVg_eEeaOq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LossPlotter(object):\n",
        "  def __init__(self, history_length):\n",
        "    self.global_steps = []\n",
        "    self.losses = []\n",
        "    self.averaged_loss_x = []\n",
        "    self.averaged_loss_y = []\n",
        "    self.history_length = history_length\n",
        "\n",
        "  def draw_plots(self):\n",
        "    self._update_averages(self.global_steps, self.losses,\n",
        "                          self.averaged_loss_x, self.averaged_loss_y)\n",
        "\n",
        "    plt.title('Average Loss Over Time')\n",
        "    plt.xlabel('Global Step')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.plot(self.averaged_loss_x, self.averaged_loss_y, label='Loss/Time (Avg)')\n",
        "    plt.plot()\n",
        "    plt.plot(self.global_steps, self.losses,\n",
        "             label='Loss/Time (Last %d)' % self.history_length,\n",
        "             alpha=.1, color='r')\n",
        "    plt.plot()\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.title('Loss for the last 100 Steps')\n",
        "    plt.xlabel('Global Step')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.plot(self.global_steps, self.losses,\n",
        "             label='Loss/Time (Last %d)' % self.history_length, color='r')\n",
        "    plt.plot()\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # The notebook will be slowed down at the end of training if we plot the\n",
        "    # entire history of raw data. Plot only the last 100 steps of raw data,\n",
        "    # and the average of each 100 batches. Don't keep unused data.\n",
        "    self.global_steps = []\n",
        "    self.losses = []\n",
        "    self.learning_rates = []\n",
        "\n",
        "  def log_step(self, global_step, loss):\n",
        "    self.global_steps.append(global_step)\n",
        "    self.losses.append(loss)\n",
        "\n",
        "  def _update_averages(self, x_list, y_list,\n",
        "                       averaged_data_x, averaged_data_y):\n",
        "    averaged_data_x.append(x_list[-1])\n",
        "    averaged_data_y.append(sum(y_list) / self.history_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CPwFDEiZhr6x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Now, we're going to start training our model.\n",
        "\n",
        "This could take a while, so you might want to grab a coffee. Every 30 seconds of training, we're going to save a checkpoint to make sure we don't lose our progress. To monitor the progress of your training, feel free to stop the training every once in a while and run the inference cell to generate text with your model!\n",
        "\n",
        "First, we will need to turn the plain text file into arrays of tokens (and, later,  back). To do this we will use this token mapper helper class:\n"
      ]
    },
    {
      "metadata": {
        "id": "n98dKVTzkmpi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "class TokenMapper(object):\n",
        "  def __init__(self):\n",
        "    self.token_mapping = {}\n",
        "    self.reverse_token_mapping = {}\n",
        "  def buildFromData(self, utf8_string, limit=0.00004):\n",
        "    print(\"Build token dictionary.\")\n",
        "    total_num = len(utf8_string)\n",
        "    sorted_tokens = sorted(Counter(utf8_string.decode('utf8')).items(), \n",
        "                           key=lambda x: -x[1])\n",
        "    # Filter tokens: Only allow printable characters (not control chars) and\n",
        "    # limit to ones that are resonably common, i.e. skip strange esoteric \n",
        "    # characters in order to reduce the dictionary size.\n",
        "    filtered_tokens = filter(lambda t: t[0] in string.printable or \n",
        "                             float(t[1])/total_num > limit, sorted_tokens)\n",
        "    tokens, counts = zip(*filtered_tokens)\n",
        "    self.token_mapping = dict(zip(tokens, range(len(tokens))))\n",
        "    for c in string.printable:\n",
        "      if c not in self.token_mapping:\n",
        "        print(\"Skipped token for: \", c)\n",
        "    self.reverse_token_mapping = {\n",
        "        val: key for key, val in self.token_mapping.items()}\n",
        "    print(\"Created dictionary: %d tokens\"%len(self.token_mapping))\n",
        "  \n",
        "  def mapchar(self, char):\n",
        "    if char in self.token_mapping:\n",
        "      return self.token_mapping[char]\n",
        "    else:\n",
        "      return self.token_mapping[' ']\n",
        "  \n",
        "  def mapstring(self, utf8_string):\n",
        "    return [self.mapchar(c) for c in utf8_string]\n",
        "  \n",
        "  def maptoken(self, token):\n",
        "    return self.reverse_token_mapping[token]\n",
        "  \n",
        "  def maptokens(self, int_array):\n",
        "    return ''.join([self.reverse_token_mapping[c] for c in int_array])\n",
        "  \n",
        "  def size(self):\n",
        "    return len(self.token_mapping)\n",
        "  \n",
        "  def alphabet(self):\n",
        "    return ''.join([k for k,v in sorted(self.token_mapping.items(),key=itemgetter(1))])\n",
        "\n",
        "  def print(self):\n",
        "    for k,v in sorted(self.token_mapping.items(),key=itemgetter(1)): print(k, v)\n",
        "  \n",
        "  def save(self, path):\n",
        "    with open(path, 'wb') as json_file:\n",
        "      json.dump(self.token_mapping, json_file)\n",
        "  \n",
        "  def restore(self, path):\n",
        "    with open(path, 'r') as json_file:\n",
        "      self.token_mapping = {}\n",
        "      self.token_mapping.update(json.load(json_file))\n",
        "      self.reverse_token_mapping = {val: key for key, val in self.token_mapping.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8XTGD7fxfEuK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now convert the raw input into a list of tokens."
      ]
    },
    {
      "metadata": {
        "id": "6RiHe0bUo9eP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "bec5e9ee-8b68-43af-eff7-74dbca8cba63"
      },
      "cell_type": "code",
      "source": [
        "# Clean the checkpoint directory and make a fresh one\n",
        "!rm -rf {CHECKPOINT_DIR}\n",
        "!mkdir {CHECKPOINT_DIR}\n",
        "!ls -lt\n",
        "\n",
        "chars_in_batch = (sequence_length * batch_size)\n",
        "file_len = len(file_contents)\n",
        "unique_sequential_batches = file_len // chars_in_batch\n",
        "\n",
        "mapper = TokenMapper()\n",
        "mapper.buildFromData(file_contents)\n",
        "mapper.save(''.join([CHECKPOINT_DIR, 'token_mapping.json']))\n",
        "\n",
        "input_values = mapper.mapstring(file_contents)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 8\r\n",
            "drwxr-xr-x 2 root root 4096 Mar  5 19:27 checkpoints\r\n",
            "drwxr-xr-x 1 root root 4096 Mar  5 18:38 datalab\r\n",
            "Build token dictionary.\n",
            "Skipped token for:  #\n",
            "Skipped token for:  $\n",
            "Skipped token for:  %\n",
            "Skipped token for:  *\n",
            "Skipped token for:  +\n",
            "Skipped token for:  /\n",
            "Skipped token for:  =\n",
            "Skipped token for:  @\n",
            "Skipped token for:  \\\n",
            "Skipped token for:  ^\n",
            "Skipped token for:  {\n",
            "Skipped token for:  ~\n",
            "Skipped token for:  \t\n",
            "Skipped token for:  \n",
            "Skipped token for:  \u000b\n",
            "Skipped token for:  \f\n",
            "Created dictionary: 84 tokens\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V5He_ECNJc61",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###First, we'll build our neural network and add our training operations to the Tensorflow graph. \n",
        "If you're continuing training after testing your generator, run the next three cells."
      ]
    },
    {
      "metadata": {
        "id": "mgXvABhpJa1f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "013f41e2-179d-459b-e298-ef270b787423"
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "print('Constructing model...')\n",
        "\n",
        "model = RNN(\n",
        "    rnn_num_layers=num_layers,\n",
        "    rnn_state_size=state_size,\n",
        "    num_classes=mapper.size(),\n",
        "    rnn_batch_size=batch_size,\n",
        "    rnn_sequence_length=sequence_length)\n",
        "\n",
        "model.build_training_model(0.05, np.asarray(input_values))\n",
        "print('Constructed model successfully.')\n",
        "\n",
        "print('Setting up training session...')\n",
        "neutral_target_weights = tf.constant(\n",
        "    np.ones(model.batch_shape),\n",
        "    tf.float32\n",
        ")\n",
        "loss = get_loss(model.logits, model.on_gpu_targets, neutral_target_weights)\n",
        "global_step = tf.get_variable('global_step', shape=(), trainable=False,\n",
        "                              dtype=tf.int32)\n",
        "training_step, computed_learning_rate = get_optimizer(\n",
        "    loss,\n",
        "    learning_rate,\n",
        "    gradient_clipping,\n",
        "    global_step,\n",
        "    steps_per_epoch,\n",
        "    learning_rate_decay\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Constructing model...\n",
            "Built LSTM:  2 256 84 64 256 (64, 256)\n",
            "Constructed model successfully.\n",
            "Setting up training session...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XNKzEmaRj5SF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The supervisor will manage the training flow and checkpointing."
      ]
    },
    {
      "metadata": {
        "id": "t90StbXJj4jt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7223b58f-b790-4a9f-f431-9eb1b0c6cc31"
      },
      "cell_type": "code",
      "source": [
        "# Create a supervisor that will checkpoint the model in the CHECKPOINT_DIR\n",
        "sv = tf.train.Supervisor(\n",
        "    logdir=CHECKPOINT_DIR,\n",
        "    global_step=global_step,\n",
        "    save_model_secs=30)\n",
        "print('Training session ready.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training session ready.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-tfXYyumK5z6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###This next cell will begin the training cycle. \n",
        "First, we will attempt to pick up training where we left off, if a previous checkpoint exists, then continue the training process."
      ]
    },
    {
      "metadata": {
        "id": "1wfsDMuLLUr3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "start_time = datetime.now()\n",
        "\n",
        "with sv.managed_session(config=config) as sess:\n",
        "  print('Training supervisor successfully initialized all variables.')\n",
        "  if not file_len:\n",
        "    raise ValueError('To continue, you must upload training data.')\n",
        "  elif file_len < chars_in_batch:\n",
        "    raise ValueError('To continue, you must upload a larger set of data.')\n",
        "\n",
        "  plotter = LossPlotter(100)\n",
        "  step_number = sess.run(global_step)\n",
        "  zero_state = sess.run([model.initial_state])\n",
        "  max_batch_index = (unique_sequential_batches - 1) * chars_in_batch\n",
        "  while not sv.should_stop() and step_number < num_training_steps:\n",
        "    feed_dict = {\n",
        "        model.batch_index: randint(0, max_batch_index),\n",
        "        model.initial_state: zero_state\n",
        "        }\n",
        "    [_, _, training_loss, step_number, current_learning_rate, _] = sess.run(\n",
        "        [model.on_gpu_sequences,\n",
        "         model.on_gpu_targets,\n",
        "         loss,\n",
        "         global_step,\n",
        "         computed_learning_rate,\n",
        "         training_step],\n",
        "        feed_dict)\n",
        "    plotter.log_step(step_number, training_loss)\n",
        "    if step_number % 100 == 0:\n",
        "      clear_output(True)\n",
        "      plotter.draw_plots()\n",
        "      print('Latest checkpoint is: %s' %\n",
        "            tf.train.latest_checkpoint(CHECKPOINT_DIR))\n",
        "      print('Learning Rate is: %f' %\n",
        "            current_learning_rate)\n",
        "\n",
        "    if step_number % 10 == 0:\n",
        "      print('global step %d, loss=%f' % (step_number, training_loss))\n",
        "\n",
        "clear_output(True)\n",
        "\n",
        "print('Training completed in HH:MM:SS = ', datetime.now()-start_time)\n",
        "print('Latest checkpoint is: %s' %\n",
        "      tf.train.latest_checkpoint(CHECKPOINT_DIR))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rpr5zK1UUlEd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Now, we're going to generate some text!\n",
        "\n",
        "Here, we'll use the **Beam Search** algorithm to generate some text with our trained model. Beam Search picks N possible next options from each of the current options at every step. This way, if the generator picks an item leading to a bad decision down the line, it can toss the bad result out and keep going with a more likely one."
      ]
    },
    {
      "metadata": {
        "id": "sc8W1I0bJy-6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BeamSearchCandidate(object):\n",
        "  \"\"\"Represents a node within the search space during Beam Search.\n",
        "\n",
        "  Attributes:\n",
        "    state: The resulting RNN state after the given sequence has been generated.\n",
        "    sequence: The sequence of selections leading to this node.\n",
        "    probability: The probability of the sequence occurring, computed as the sum\n",
        "      of the probabilty of each character in the sequence at its respective\n",
        "      step.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, init_state, sequence, probability):\n",
        "    self.state = init_state\n",
        "    self.sequence = sequence\n",
        "    self.probability = probability\n",
        "\n",
        "  def search_from(self, tf_sess, rnn_model, temperature, num_options):\n",
        "    \"\"\"Expands the num_options most likely next elements in the sequence.\n",
        "\n",
        "    Args:\n",
        "      tf_sess: The Tensorflow session containing the rnn_model.\n",
        "      rnn_model: The RNN to use to generate the next element in the sequence.\n",
        "      temperature: Modifies the probabilities of each character, placing\n",
        "        more emphasis on higher probabilities as the value approaches 0.\n",
        "      num_options: How many potential next options to expand from this one.\n",
        "\n",
        "    Returns: A list of BeamSearchCandidate objects descended from this node.\n",
        "    \"\"\"\n",
        "    expanded_set = []\n",
        "    feed = {rnn_model.input_symbol: np.array([[self.sequence[-1]]]),\n",
        "            rnn_model.initial_state: self.state,\n",
        "            rnn_model.temperature: temperature,\n",
        "            rnn_model.num_options: num_options}\n",
        "    [predictions, probabilities, new_state] = tf_sess.run(\n",
        "        [rnn_model.output_labels,\n",
        "         rnn_model.normalized_probs,\n",
        "         rnn_model.new_state], feed)\n",
        "    # Get the indices of the num_beams next picks\n",
        "    picks = [predictions[0][x] for x in range(len(predictions[0]))]\n",
        "    for new_char in picks:\n",
        "      new_seq = deepcopy(self.sequence)\n",
        "      new_seq.append(new_char)\n",
        "      expanded_set.append(\n",
        "          BeamSearchCandidate(new_state, new_seq,\n",
        "                              probabilities[0][0][new_char] + self.probability))\n",
        "    return expanded_set\n",
        "\n",
        "  def __eq__(self, other):\n",
        "    return self.sequence == other.sequence\n",
        "\n",
        "  def __ne__(self, other):\n",
        "    return not self.__eq__(other)\n",
        "\n",
        "  def __hash__(self):\n",
        "    return hash(self.sequence())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "97aGkvJGNFUH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def beam_search_generate_sequence(tf_sess, rnn_model, primer, temperature=0.85,\n",
        "                                  termination_condition=None, num_beams=5):\n",
        "  \"\"\"Implements a sequence generator using Beam Search.\n",
        "\n",
        "  Args:\n",
        "    tf_sess: The Tensorflow session containing the rnn_model.\n",
        "    rnn_model: The RNN to use to generate the next element in the sequence.\n",
        "    temperature: Controls how 'Creative' the generated sequence is. Values\n",
        "      close to 0 tend to generate the most likely sequence, while values\n",
        "      closer to 1 generate more original sequences. Acceptable values are\n",
        "      within (0, 1].\n",
        "    termination_condition: A function taking one parameter, a list of\n",
        "      integers, that returns True when a condition is met that signals to the\n",
        "      RNN to return what it has generated so far.\n",
        "    num_beams: The number of possible sequences to keep at each step of the\n",
        "      generation process.\n",
        "\n",
        "  Returns: A list of at most num_beams BeamSearchCandidate objects.\n",
        "  \"\"\"\n",
        "  candidates = []\n",
        "\n",
        "  rnn_current_state = sess.run([rnn_model.initial_state])\n",
        "  #Initialize the state for the primer\n",
        "  for primer_val in primer[:-1]:\n",
        "    feed = {rnn_model.input_symbol: np.array([[primer_val]]),\n",
        "            rnn_model.initial_state: rnn_current_state\n",
        "           }\n",
        "    [rnn_current_state] = tf_sess.run([rnn_model.new_state], feed)\n",
        "\n",
        "  candidates.append(BeamSearchCandidate(rnn_current_state, primer, num_beams))\n",
        "\n",
        "  while True not in [termination_condition(x.sequence) for x in candidates]:\n",
        "    new_candidates = []\n",
        "    for candidate in candidates:\n",
        "      expanded_candidates = candidate.search_from(\n",
        "          tf_sess, rnn_model, temperature, num_beams)\n",
        "      for new in expanded_candidates:\n",
        "        if new not in new_candidates:\n",
        "          #do not reevaluate duplicates\n",
        "          new_candidates.append(new)\n",
        "    candidates = sorted(new_candidates,\n",
        "                        key=lambda x: x.probability, reverse=True)[:num_beams]\n",
        "\n",
        "  return [c for c in candidates if termination_condition(c.sequence)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qj0w_Vgs-oMJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Input something to start your generated text with, and set how characters long you want the text to be.\n",
        "\"Creativity\" refers to how much emphasis your neural network puts on matching a pattern. If you notice looping in the output, try raising this value. If your output seems too random, try lowering it a bit.\n",
        "If the results don't look too great in general, run the three training cells again for a bit longer. The lower your loss, the more closely your generated text will match the training data."
      ]
    },
    {
      "metadata": {
        "id": "rAsyTCZvdFn5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "87a40abc-41a7-46f2-d62c-df0628a0e253"
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf.InteractiveSession(config=config)\n",
        "\n",
        "model = RNN(\n",
        "    rnn_num_layers=num_layers,\n",
        "    rnn_state_size=state_size,\n",
        "    num_classes=mapper.size(),\n",
        "    rnn_batch_size=1,\n",
        "    rnn_sequence_length=1)\n",
        "\n",
        "model.build_inference_model()\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "saver = tf.train.Saver(tf.global_variables())\n",
        "ckpt = tf.train.latest_checkpoint(CHECKPOINT_DIR)\n",
        "saver.restore(sess, ckpt)\n",
        "\n",
        "def gen(start_with, pred, creativity):\n",
        "  int_array = mapper.mapstring(start_with)\n",
        "  candidates = beam_search_generate_sequence(\n",
        "      sess, model, int_array, temperature=creativity,\n",
        "      termination_condition=pred,\n",
        "      num_beams=1)\n",
        "  gentext = mapper.maptokens(candidates[0].sequence)\n",
        "  return gentext\n",
        "\n",
        "def lengthlimit(n):\n",
        "  return lambda text: len(text)>n\n",
        "def sentences(n):\n",
        "  return lambda text: mapper.maptokens(text).count(\".\")>=n\n",
        "def paragraph():\n",
        "  return lambda text: mapper.maptokens(text).count(\"\\n\")>0\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Built LSTM:  2 256 84 1 1 (1, 1)\n",
            "INFO:tensorflow:Restoring parameters from ./checkpoints/model.ckpt-7270\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "09ccehpqVKR8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "cellView": "both",
        "outputId": "3bc5752a-9d8e-4ac9-dbc0-edb1fd2cf64c"
      },
      "cell_type": "code",
      "source": [
        "length_of_generated_text = 2000\n",
        "creativity = 0.85  # Should be greater than 0 but less than 1\n",
        "\n",
        "print(gen(\"  ANTONIO: Who is it ?\", lengthlimit(length_of_generated_text), creativity))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  ANTONIO: Who is it ?IE. Do piy min, wher by till blingestn.\n",
            "    Mave in wind for ne cient hafesteres one for yor your yaev yould and londond, afrely,,\n",
            "    At that Wood tho your you, waecth wing Cichbry your,\n",
            "    I theer mien, ward the me a see hen thy yould berliiver to her mesting of rive ore yout,\n",
            "     Thoush F'er in helr all hive und so bing the nost atHer with stourss the madss'\n",
            "    Kom no, you shat with thing in shee and wear lat yom horch.\n",
            "    Aqhat wirte you dose ou? 'con acrerante,\n",
            "    Fow his nast sthe, lost the sarthind.\n",
            "    Read und is liubk.                                                                         Anmen swenely they for musmer, with the woruster. Aurles I I hell with she laikn.\n",
            "    Mush me  tid. Why will nle bes by lothile Vith is entern-that recume thee, anlind on whou to rudes\n",
            "    Bust a hath tull a pnane form gowet proncud, here and thou theat sam,\n",
            "    Tull at the me then with you sowe braen wune\n",
            "    Susralshes to gresheneuy, and tleacenss\n",
            "    thall a the pids my goon and so me els coon fathire\n",
            "    And prove, but anderers\n",
            "    will for gromen hit, suik, in that, food of thee shoundly;\n",
            "    Luttarr my tutt frat yoe?\n",
            "  NUBITORSNO. What whe vay ip withel. Sretels ase lotst a pold,\n",
            "    The shigh old hath not swamure to have\n",
            "    And your for youl cuetin serserby,\n",
            "    And gofe wham wis it whoud thith hesw of in wour.\n",
            "  COLID. For at tall yourd and shaede no hach bethe,\n",
            "    and that maty tood beulder, arld me sende ingee wit te dreike\n",
            "    I to seever four bud so hen? cithrer, lesthitee. worlmy me thave peralion;\n",
            "    Gut be's, hiw woR brafans on now shat blomelf wouk,\n",
            "    Home sithetht\n",
            "    To dutwell wills in wo tege me noptnes?\n",
            "    Whaw so fallers, doed athise sist to mech not on, hither we 'tund, thoun\n",
            "    Noald of wamst is stinle broulk.\n",
            "  HACATIS. Hy weach,\n",
            "    Hithen coluntange bryake, there your wast whave borje ould. I tot at\n",
            "    Sore his the lat apowine i  mest come.\n",
            "  POCKEN. Yich to shaghed as sele the, youd peesiter be, is now rasol.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "klTwA-RrFwR0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Let's save a copy of our trained RNN so we can do all kinds of cool things with it later."
      ]
    },
    {
      "metadata": {
        "id": "hdxjJaayFuhS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a80fc89e-6e86-4e1e-9a4d-c2ed173e8ba4"
      },
      "cell_type": "code",
      "source": [
        "save_model_to_drive = False  ## Set this to true to save directly to Google Drive.\n",
        "\n",
        "def save_model_hyperparameters(path):\n",
        "  with open(path, 'w')  as json_file:\n",
        "    model_params = {\n",
        "        'num_layers': model.num_layers,\n",
        "        'state_size': model.state_size,\n",
        "        'num_classes': model.num_classes\n",
        "    }\n",
        "    json.dump(model_params, json_file)\n",
        "\n",
        "def save_to_drive(title, content):\n",
        "  # Install the PyDrive wrapper & import libraries.\n",
        "  !pip install -U -q PyDrive\n",
        "  from pydrive.auth import GoogleAuth\n",
        "  from pydrive.drive import GoogleDrive\n",
        "  from google.colab import auth\n",
        "  from oauth2client.client import GoogleCredentials\n",
        "\n",
        "  # Authenticate and create the PyDrive client.\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "\n",
        "  newfile = drive.CreateFile({'title': title})\n",
        "  newfile.SetContentFile(content)\n",
        "  newfile.Upload()\n",
        "  print('Uploaded file with ID %s as %s'% (newfile.get('id'),\n",
        "         archive_name))\n",
        "    \n",
        "archive_name = ''.join([file_name,'_seedbank_char-rnn.zip'])\n",
        "latest_model = tf.train.latest_checkpoint(CHECKPOINT_DIR).split('/')[2]\n",
        "checkpoints_archive_path = ''.join(['./exports/',archive_name])\n",
        "if not latest_model:\n",
        "  raise ValueError('You must train a model before you can export one.')\n",
        "  \n",
        "%system mkdir exports\n",
        "%rm -f {checkpoints_archive_path}\n",
        "mapper.save(''.join([CHECKPOINT_DIR, 'token_mapping.json']))\n",
        "save_model_hyperparameters(''.join([CHECKPOINT_DIR, 'model_attributes.json']))\n",
        "%system zip '{checkpoints_archive_path}' -@ '{CHECKPOINT_DIR}checkpoint' \\\n",
        "            '{CHECKPOINT_DIR}token_mapping.json' \\\n",
        "            '{CHECKPOINT_DIR}model_attributes.json' \\\n",
        "            '{CHECKPOINT_DIR}{latest_model}.'*\n",
        "\n",
        "if save_model_to_drive:\n",
        "  save_to_drive(archive_name, checkpoints_archive_path)\n",
        "else:\n",
        "  files.download(checkpoints_archive_path)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded file with ID 1dQEv67yQe10ccsW13sJx89ilhCDmXzxP as shakespeare_seedbank_char-rnn.zip\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}